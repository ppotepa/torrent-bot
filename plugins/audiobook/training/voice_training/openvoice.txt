Brilliant — here’s a **complete, OpenVoice-only** playbook you can follow end-to-end. It focuses on **OpenVoice V2** (better audio, native multilingual support, MIT licence) and shows exactly how to run **zero-shot voice cloning** from your own reference samples and generate audio from any text. I’ll also include a minimal voice-conversion (speech-to-speech) snippet at the end.

---

# What you get (at a glance)

* **Instant voice cloning** from a short **reference audio** (your voice) – no speaker training needed.
* **Control** over **language**, **accent (via base speakers)** and **pace**; tone colour comes from your reference, style from the base speaker.
* **Supported languages (V2 native):** English, Spanish, French, Chinese, Japanese, Korean. Cross-lingual cloning is core to OpenVoice’s approach; V1 demos show unseen-language cloning too. ([Hugging Face][1], [MyShell Research][2])
* **Licence:** MIT, **free for commercial use** (V1 & V2). ([GitHub][3])

---

# 0) Hardware & audio tips

* **GPU recommended** (NVIDIA ≥8 GB VRAM). CPU and Apple Silicon (MPS) also work, just slower.
* **Reference audio:** clean mic, minimal reverb, **15–60 s** is usually fine; longer clips can help.
* OpenVoice **clones tone colour**; **accent/emotion are not cloned** (pick a base speaker/style instead). ([GitHub][4])

---

# 1) Install (V2)

### Linux (official path)

```bash
# 1) Conda env (recommended by authors)
conda create -n openvoice python=3.9 -y
conda activate openvoice

# 2) Get OpenVoice and install it editable
git clone https://github.com/myshell-ai/OpenVoice.git
cd OpenVoice
pip install -e .

# 3) Install MeloTTS (required by V2) + Japanese dictionary for tokeniser
pip install "git+https://github.com/myshell-ai/MeloTTS.git"
python -m unidic download
```

Now **download V2 checkpoints** and unzip into `checkpoints_v2/` at the repo root:

```bash
wget https://myshell-public-repo-hosting.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip
unzip checkpoints_v2_0417.zip -d checkpoints_v2
```

(These steps and links are from the official V2 readme.) ([Hugging Face][1])

### Windows (community guide)

If you’re on Windows, follow the community Windows install (Conda + ffmpeg + same steps as above). The project links to a maintained Windows guide. ([Hugging Face][1])

---

# 2) Folder structure (after setup)

```
OpenVoice/
  checkpoints_v2/
    base_speakers/
      ses/
        en-us.pth
        en-india.pth
        en-au.pth
        ...
    converter/
      config.json
      checkpoint.pth
  resources/
    example_reference.mp3
  openvoice/...
  melo/... (installed in site-packages)
```

V2 works like this: **MeloTTS** synthesises content in your chosen language/accent using a **base speaker**, then **ToneColorConverter** transfers **your voice timbre** onto that audio. (This decoupling is why you control style separately from your voice.) ([Hugging Face][1])

---

# 3) Minimal zero-shot TTS (V2)

Save as `infer_v2.py` in the repo root:

```python
import os, sys, torch
from openvoice import se_extractor
from openvoice.api import ToneColorConverter
from melo.api import TTS

# ---- settings ----
TEXT = "Cześć! To jest szybki test OpenVoice w trybie zero-shot."
REFERENCE_WAV = "resources/example_reference.mp3"   # put your WAV/MP3 path here
LANG = "EN"  # EN, ES, FR, ZH, JP, KR (V2 native)
OUT_WAV = "out_openvoice_v2.wav"
TMP_SRC = "tmp_base.wav"

# pick device
if torch.cuda.is_available():
    DEVICE = "cuda:0"
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    DEVICE = "mps"
else:
    DEVICE = "cpu"

# ---- load converter (timbre transfer) ----
ckpt_root = "checkpoints_v2"
ckpt_converter = os.path.join(ckpt_root, "converter")
converter = ToneColorConverter(os.path.join(ckpt_converter, "config.json"), device=DEVICE)
converter.load_ckpt(os.path.join(ckpt_converter, "checkpoint.pth"))

# extract your target timbre embedding from reference audio
tgt_se, _ = se_extractor.get_se(REFERENCE_WAV, converter, vad=True)

# ---- synthesise base audio in target language with MeloTTS ----
tts = TTS(language=LANG, device=DEVICE)

# list available base speakers for this language (accents/styles)
print("Available base speakers:", list(tts.hps.data.spk2id.keys()))

# choose one (e.g., first English speaker)
def pick_speaker(spk2id, lang):
    # prefer keys containing the language code, else first
    for k in spk2id.keys():
        if lang in k:
            return k
    return next(iter(spk2id.keys()))

base_spk_key = pick_speaker(tts.hps.data.spk2id, LANG)
base_spk_id = tts.hps.data.spk2id[base_spk_key]
print("Using base speaker:", base_spk_key)

# speed can be adjusted (1.0 = normal)
tts.tts_to_file(TEXT, speaker_id=base_spk_id, file_path=TMP_SRC, speed=1.0)

# ---- load source (base) timbre embedding for the chosen speaker ----
src_se_path = os.path.join(ckpt_root, "base_speakers", "ses", f"{base_spk_key.lower().replace('_','-')}.pth")
source_se = torch.load(src_se_path, map_location=DEVICE)

# ---- convert base audio timbre -> your voice timbre ----
converter.convert(
    audio_src_path=TMP_SRC,
    src_se=source_se,
    tgt_se=tgt_se,
    output_path=OUT_WAV,
    message="@MyShell"  # watermark message used in examples
)
print("Saved:", OUT_WAV)
```

Run:

```bash
conda activate openvoice
python infer_v2.py
```

> Notes
> • **Change `LANG`** to pick language (EN/ES/FR/ZH/JP/KR).
> • The `print(...)` shows available **base speakers** (e.g. `EN-US`, `EN-INDIA`, `EN-AU`), which influence accent/prosody.
> • This flow (MeloTTS → ToneColorConverter) mirrors the official V2 demos and community examples. ([Hugging Face][1], [Qiita][5])

---

# 4) Batch generation

```python
# generate one file per line from input.txt
with open("input.txt", encoding="utf-8") as f:
    for i, line in enumerate(f, 1):
        text = line.strip()
        if not text:
            continue
        # set TEXT = text and OUT_WAV = f"batch_{i:03d}.wav"
        # then call the same pipeline as in infer_v2.py
```

---

# 5) Optional: Voice conversion (speech-to-speech)

Convert the **timbre** of a *source* recording (keeps content/timing) into your voice:

```python
# given: converter loaded, DEVICE set as above

SOURCE_AUDIO = "narrator.wav"       # content you want to re-voice
REFERENCE_WAV = "my_voice_ref.wav"  # your voice sample

src_se, _ = se_extractor.get_se(SOURCE_AUDIO, converter, vad=True)
tgt_se, _ = se_extractor.get_se(REFERENCE_WAV, converter, vad=True)

converter.convert(
    audio_src_path=SOURCE_AUDIO,
    src_se=src_se,
    tgt_se=tgt_se,
    output_path="converted_to_my_voice.wav",
    message="@MyShell"
)
```

This usage (speech-to-speech / voice-changer) aligns with community Q\&A and the converter API design. ([GitHub][6])

---

# 6) Controlling style & quality

* **Accent & style:** choose a different **base speaker** key (e.g. `EN-INDIA`, `EN-AU`, etc.) when synthesising with MeloTTS before conversion. ([Qiita][5])
* **Speed:** pass `speed=...` to `tts.tts_to_file(...)`. ([Qiita][5])
* **Pacing & punctuation:** commas/periods help natural pauses; some users insert ellipses `…` or split long text into sentences for steadier prosody.
* **Polish or other non-native languages:** V2 ships native support for 6 languages. For other languages, OpenVoice’s approach supports **zero-shot cross-lingual cloning** (demonstrated in V1 resources). Expect results to vary and try different base speakers or small text rewrites. ([MyShell Research][7], [arXiv][8])

---

# 7) Troubleshooting

* **“Doesn’t sound like me”** → use a longer, cleaner **reference** (30–90 s), avoid music/background noise; try another base speaker; ensure `vad=True` in `get_se`.
* **Accent/emotion aren’t copied** → that’s expected; tone colour is cloned, **accent/emotion come from base speaker / text**. ([GitHub][4])
* **Install issues** → re-check Python 3.9 env, reinstall MeloTTS & run `python -m unidic download`; on Windows follow the linked guide. ([Hugging Face][1])
* **Performance** → use CUDA; for Mac, MPS may help; split very long paragraphs into sentences.

---

# 8) Quick demo options

* **Local minimal Gradio (V1)**: `python -m openvoice_app --share` (useful to sanity-check your setup).
* **Hugging Face Space (V2)**: hosted demos available. ([Hugging Face][1])

---

# 9) Licence & usage

OpenVoice **V1 & V2 are MIT-licensed** and “free for commercial use.” Always clone **only voices you own or have clear permission for**. ([GitHub][3])

---

## Sources

* Official OpenVoice repo (MIT; V1+V2 overview, usage pointers). ([GitHub][3])
* OpenVoice **V2** model card & **usage** (checkpoints link, MeloTTS setup, demo). ([Hugging Face][1])
* Research page with **flexible style** and **zero-shot cross-lingual** examples. ([MyShell Research][2])
* **OpenVoice paper** (approach & capabilities). ([arXiv][9])
* Community example showing the **V2 pipeline** (MeloTTS + ToneColorConverter + `se_extractor`). ([Qiita][5])

---

If you want, I can package this into a tiny **one-command repo** for you (`prepare`, `infer`, `convert`, `batch`) so your team can drop in a reference WAV/MP3 and generate lines straight away.

[1]: https://huggingface.co/myshell-ai/OpenVoiceV2/blame/d1898cb74618b01a586a411a1f4e6feca24eba94/README.md "README.md · myshell-ai/OpenVoiceV2 at d1898cb74618b01a586a411a1f4e6feca24eba94"
[2]: https://research.myshell.ai/open-voice?utm_source=chatgpt.com "OpenVoice: Versatile Instant Voice Cloning | MyShell AI"
[3]: https://github.com/myshell-ai/OpenVoice "GitHub - myshell-ai/OpenVoice: Instant voice cloning by MIT and MyShell. Audio foundation model."
[4]: https://github.com/myshell-ai/OpenVoice/issues/232?utm_source=chatgpt.com "v2 and emotion / precise voice control. · Issue #232"
[5]: https://qiita.com/goofmint/items/cad9d97ba3dad64b1c94 "DOK × OpenVoiceで音声クローンを試す #Python - Qiita"
[6]: https://github.com/myshell-ai/OpenVoice/issues/315?utm_source=chatgpt.com "can I use openvoice for speech to speech · Issue #315"
[7]: https://research.myshell.ai/open-voice/zero-shot-cross-lingual-voice-cloning?utm_source=chatgpt.com "Zero-shot Cross-lingual Voice Cloning | MyShell AI - OpenVoice"
[8]: https://arxiv.org/html/2312.01479v5?utm_source=chatgpt.com "OpenVoice: Versatile Instant Voice Cloning"
[9]: https://arxiv.org/abs/2312.01479?utm_source=chatgpt.com "OpenVoice: Versatile Instant Voice Cloning"


Not natively, no.

👉 **OpenVoice V2** only has *built-in support* for **English, Spanish, French, Chinese, Japanese, and Korean**.

However:

* The authors explicitly show **cross-lingual zero-shot voice cloning** (using your voice timbre with text in “unseen” languages).
* That means you *can* try Polish text: the timbre transfer will still work, but pronunciation quality depends on the **base speaker** model. Since there is no native Polish base speaker in V2, you may hear accent issues, mispronounced words, or English-like phoneme mapping.
* In V1 demos, people have reported some success with unsupported languages by experimenting with different base speakers and careful spelling tweaks.

So:

* **Polish isn’t officially supported.**
* **You can test it in zero-shot mode**, but results will vary (it may sound “accented” or garbled on tricky words).
* For consistent Polish TTS, you’d want to combine OpenVoice’s *voice cloning* with a separate **Polish TTS backend** (e.g. Coqui XTTS, Piper, or mTTS), then apply OpenVoice’s **ToneColorConverter** to transfer your timbre.

Do you want me to sketch out how you could **pipeline OpenVoice with a Polish TTS engine** so you get proper Polish pronunciation *and* your cloned voice?
