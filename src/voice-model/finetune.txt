Świetnie — skoro **OpenVoice** już działa u Ciebie, zróbmy z niego **porządny polski pipeline**. Klucz: używamy OpenVoice wyłącznie do **transferu barwy** (ToneColorConverter), a **polski TTS** generujemy „pod spodem” innym silnikiem (XTTS lub Piper). To eliminuje „robotyczność”, bo fonetyka PL będzie robiona natywnie.

Poniżej masz **precyzyjne, krok-po-kroku** instrukcje + gotowe skrypty.

---

# Plan w skrócie

1. Zaktualizuj OpenVoice V2 + checkpointy (jeśli nie masz najnowszych). ([Hugging Face][1])
2. Przygotuj porządne **próbki referencyjne** i wyciągnij stabilne embeddingi (agregacja kilku próbek).
3. Wybierz backend PL:

   * **A. Coqui XTTS-v2 (polecane)** – PL jest natywnie wspierany. ([Hugging Face][2])
   * **B. Piper (lekki, lokalny)** – dobierz polski głos, np. *pl\_PL/gosia/medium*. ([Hugging Face][3], [GitHub][4])
4. **Wygeneruj bazowe audio po polsku** (XTTS/Piper).
5. **Przenieś barwę** Twojego głosu na to audio przez **ToneColorConverter** (OpenVoice). (OpenVoice wprost wspiera speech-to-speech/voice-changer). ([GitHub][5])

> Uwaga o parametrach audio: konwerter OpenVoice przyjmuje wejście **\~22 050 Hz** — trzymajmy się tego resamplując bazowe audio i próbki. ([NVIDIA NGC Catalog][6])

---

## Krok 0 — upewnij się, że masz poprawne V2

```bash
# w repo OpenVoice
git pull
pip install -e .
pip install "git+https://github.com/myshell-ai/MeloTTS.git"
python -m unidic download

# checkpointy V2
wget https://myshell-public-repo-hosting.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip
unzip checkpoints_v2_0417.zip -d checkpoints_v2
```

(Według oficjalnego README dla V2). ([Hugging Face][1])

---

## Krok 1 — przygotuj referencje (Twoje próbki)

1. Zbierz **30–90 s** czystej mowy (2–8 klipów).
2. Ujednolić format (mono, **22 050 Hz**, 16-bit WAV):

```bash
ffmpeg -i in.wav -ac 1 -ar 22050 -sample_fmt s16 out_22050.wav
```

3. **Wyodrębnij stabilny embedding** barwy z wielu próbek (średnia):

```python
# scripts/ref_embed_pl.py
import torch, glob, os
from openvoice.api import ToneColorConverter
from openvoice import se_extractor

REF_DIR = "ref_pl"  # tu wrzuć swoje .wav/.mp3 (czyste, 22.05 kHz)
CKPT = "checkpoints_v2/converter"

device = "cuda:0" if torch.cuda.is_available() else "cpu"
conv = ToneColorConverter(os.path.join(CKPT, "config.json"), device=device)
conv.load_ckpt(os.path.join(CKPT, "checkpoint.pth"))

embs = []
for p in glob.glob(os.path.join(REF_DIR, "*")):
    se, _ = se_extractor.get_se(p, conv, vad=True)  # VAD przycina ciszę
    embs.append(se)

avg = torch.mean(torch.stack(embs), dim=0)
os.makedirs("cache", exist_ok=True)
torch.save(avg, "cache/tgt_se_avg.pth")
print("Zapisano: cache/tgt_se_avg.pth")
```

`se_extractor.get_se(..., vad=True)` to rekomendowany sposób pobierania embeddingu w OpenVoice. ([Hugging Face][7])

---

## Krok 2A — backend PL: **Coqui XTTS-v2** (polecane)

Instalacja:

```bash
pip install TTS
```

Minimalny generator polskiego audio:

```python
# scripts/base_pl_xtts.py
from TTS.api import TTS

def synth_pl(text: str, out_wav: str):
    tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=True)
    # generujemy "neutralny" polski głos – bez speaker_wav
    tts.tts_to_file(text=text, file_path=out_wav, language="pl", split_sentences=True)

if __name__ == "__main__":
    synth_pl("To jest bazowe nagranie po polsku do konwersji barwy.", "tmp_base_pl.wav")
    print("OK: tmp_base_pl.wav")
```

XTTS-v2 ma natywne **pl** w liście języków (co rozwiązuje fonetykę). ([Hugging Face][2])

---

## Krok 2B — backend PL: **Piper** (lekki, offline)

Instalacja (najprościej):

```bash
python -m venv .venv && source .venv/bin/activate
pip install piper-tts
```

Pobierz polski głos, np. **pl\_PL/gosia/medium** (22 050 Hz). Zobacz model-card i listę głosów. ([Hugging Face][3], [Rhasspy][8])

Przykładowa generacja (CLI):

```bash
# zakładamy, że masz pliki: pl_PL-gosia-medium.onnx i odpowiadający .json
echo "To jest bazowy TTS po polsku." | piper \
  --model ./pl_PL-gosia-medium.onnx \
  --output_file tmp_base_pl.wav
```

(Jeśli wolisz 100% Pythonem, `piper-tts` udostępnia API i skrypt `piper`; bywa różnica wersji — gdyby CLI krzyczało o opcje/model, skorzystaj z wydań i README projektu). ([PyPI][9], [GitHub][4], [Unix & Linux Stack Exchange][10])

> W obu backendach (XTTS/Piper) na końcu **upewnij się o 22 050 Hz**:
> `ffmpeg -i tmp_base_pl.wav -ac 1 -ar 22050 -sample_fmt s16 tmp_base_pl_22050.wav` (OpenVoice converter). ([NVIDIA NGC Catalog][6])

---

## Krok 3 — **transfer barwy** w OpenVoice (PL audio → Twój timbre)

```python
# scripts/ov_convert_pl.py
import os, torch
from openvoice.api import ToneColorConverter
from openvoice import se_extractor

DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
CKPT = "checkpoints_v2/converter"

BASE_WAV = "tmp_base_pl_22050.wav"   # bazowy polski TTS (XTTS/Piper), 22.05 kHz
TGT_SE = "cache/tgt_se_avg.pth"      # embedding z Krok 1 (Twoje próbki)
OUT = "out_polish_myvoice.wav"

conv = ToneColorConverter(os.path.join(CKPT,"config.json"), device=DEVICE)
conv.load_ckpt(os.path.join(CKPT,"checkpoint.pth"))

# źródłowy embedding najlepiej wyciągnąć bezpośrednio z bazowego audio
src_se, _ = se_extractor.get_se(BASE_WAV, conv, vad=True)
tgt_se = torch.load(TGT_SE, map_location=DEVICE)

conv.convert(
    audio_src_path=BASE_WAV,
    src_se=src_se,
    tgt_se=tgt_se,
    output_path=OUT,
    message="@MyShell"
)
print("Zapisano:", OUT)
```

OpenVoice oficjalnie wspiera taki **speech-to-speech** (voice changer) scenariusz. ([GitHub][5])

---

## Krok 4 — jeden skrypt „wszystko w jednym” (pipeline)

```python
# scripts/pipeline_pl.py
import os, sys, argparse, subprocess

def run(cmd):
    print("+", " ".join(cmd)); subprocess.run(cmd, check=True)

p = argparse.ArgumentParser()
p.add_argument("--text", required=True)
p.add_argument("--backend", choices=["xtts","piper"], default="xtts")
p.add_argument("--out", default="out_polish_myvoice.wav")
args = p.parse_args()

# 1) bazowy PL TTS
if args.backend == "xtts":
    run([sys.executable, "scripts/base_pl_xtts.py"])  # zapisze tmp_base_pl.wav
else:
    # przykład z Piper CLI (dostosuj ścieżkę do .onnx)
    textfile = "tmp.txt"
    open(textfile,"w",encoding="utf-8").write(args.text)
    run(["bash","-lc",
         f"cat {textfile} | piper --model ./pl_PL-gosia-medium.onnx --output_file tmp_base_pl.wav"])

# 2) resampling do 22.05 kHz
run(["ffmpeg","-y","-i","tmp_base_pl.wav","-ac","1","-ar","22050","-sample_fmt","s16","tmp_base_pl_22050.wav"])

# 3) konwersja barwy
run([sys.executable, "scripts/ov_convert_pl.py"])

# 4) przenieś na docelową nazwę
os.replace("out_polish_myvoice.wav", args.out)
print("Gotowe:", args.out)
```

---

## Krok 5 — „dlaczego wcześniej brzmiało robotycznie?”

* OpenVoice **nie ma natywnego PL** — bazowy generator (MeloTTS w V2) nie posiada polskich fonemów, więc z tekstu PL robi „aproksymację”, skutkując **metalicznością/bezładną artykulacją**. Rozdzielenie: „polski TTS” → *osobny silnik* + **ToneColorConverter** → *Twoja barwa* rozwiązuje ten problem. (README V2 i repo jasno pokazują, że V2 to „MeloTTS → Converter”). ([Hugging Face][1])

---

## Strojenie jakości (ważne, precyzyjne)

1. **Próbki referencyjne**

   * 2–8 klipów, razem **≥ 30 s** (lepiej 60–120 s), bez szumów, stały mikrofon, brak kompresji.
   * Zawsze **22 050 Hz**, mono, 16-bit.
   * Używaj `vad=True` przy ekstrakcji i **średniej embeddingów** (patrz Krok 1). ([Hugging Face][7])

2. **Tekst i prozodia**

   * Dziel długie akapity na zdania/kropki/ przecinki; to pomaga rytmowi.
   * Liczby słownie („2025” → „dwa tysiące dwadzieścia pięć”), skróty rozwijaj.
   * W **Piper** możesz regulować tempo `--length_scale 1.0` (mniejsze = szybciej).
   * W **XTTS** dziel zdania (`split_sentences=True`) i ewentualnie generuj partiami.

3. **Głośność i szczyty**

   * Po konwersji możesz znormalizować do \~-1 dBFS:
     `ffmpeg -i out.wav -filter:a "volume=1.5" out_loud.wav` (dostosuj).

4. **Zbieżność barwy**

   * Jeśli barwa „pływa”, zwiększ liczbę próbek lub użyj dłuższych (do \~90 s łącznie).
   * Zadbaj, by próbki zawierały naturalne pauzy i różną intonację.

---

## Batch / wsad

```bash
# każdy wiersz w input.txt -> osobny plik
while IFS= read -r line; do
  python scripts/pipeline_pl.py --text "$line" --backend xtts --out "polish_${RANDOM}.wav"
done < input.txt
```

---

## Najczęstsze problemy

* **„Nadal brzmi robotycznie”** – sprawdź, czy bazowe audio naprawdę generujesz **polskim TTS** (XTTS/Piper), a nie MeloTTS z V2; sprawdź też resampling do 22 050 Hz przed konwersją. ([NVIDIA NGC Catalog][6])
* **„Syczenie/artefakty”** – użyj czystszych referencji, dołóż 1–2 klipy, włącz VAD przy ekstrakcji, znormalizuj wejścia. ([Hugging Face][7])
* **„CLI Piper marudzi”** – wersje `piper-tts` miewają różne opcje zależnie od buildów; bazuj na repo/README i głosach z rhasspy/piper-voices. ([PyPI][9], [GitHub][4])

---

## Dlaczego to działa

* **OpenVoice V2**: architektura „**MeloTTS (base)** → **ToneColorConverter** (transfer barwy)”. My tylko podmieniamy część „base” na **polski TTS**, a konwerter robi swoje — przenosi **Twoją barwę** na **polską fonetykę**. (Taką kompozycję przewidują materiały V2; konwerter działa z dowolnym wejściem audio). ([Hugging Face][1], [GitHub][5])

---

### Źródła

* OpenVoice V2 README (instalacja, checkpointy, zależność od MeloTTS). ([Hugging Face][1])
* Coqui **XTTS-v2** – lista języków (z PL). ([Hugging Face][2])
* Piper: polskie głosy (np. *pl\_PL/gosia/medium*), próbki. ([Hugging Face][3], [Rhasspy][8])
* OpenVoice: voice-changer / speech-to-speech (konwerter). ([GitHub][5])
* OpenVoice NGC: wejście 22 050 Hz dla konwertera. ([NVIDIA NGC Catalog][6])
* se\_extractor: użycie z VAD w praktyce. ([Hugging Face][7])

---

Jeśli chcesz, mogę dorzucić **gotowe pliki** (`ref_embed_pl.py`, `base_pl_xtts.py`, `ov_convert_pl.py`, `pipeline_pl.py`) pod Twój layout repo — powiedz tylko, czy stawiamy na **XTTS** czy **Piper** jako backend.

[1]: https://huggingface.co/myshell-ai/OpenVoiceV2/blame/d1898cb74618b01a586a411a1f4e6feca24eba94/README.md?utm_source=chatgpt.com "README.md · myshell-ai/OpenVoiceV2 at ..."
[2]: https://huggingface.co/coqui/XTTS-v2?utm_source=chatgpt.com "coqui/XTTS-v2"
[3]: https://huggingface.co/rhasspy/piper-voices/blob/058271fb41b630e96989367e15b4514992a25b42/pl/pl_PL/gosia/medium/MODEL_CARD?utm_source=chatgpt.com "pl/pl_PL/gosia/medium/MODEL_CARD - piper-voices"
[4]: https://github.com/rhasspy/piper?utm_source=chatgpt.com "rhasspy/piper: A fast, local neural text to speech system"
[5]: https://github.com/myshell-ai/OpenVoice/issues/315?utm_source=chatgpt.com "can I use openvoice for speech to speech · Issue #315"
[6]: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nvigisdk/models/openvoice?utm_source=chatgpt.com "OpenVoice | NVIDIA NGC"
[7]: https://huggingface.co/myshell-ai/OpenVoice/discussions/7?utm_source=chatgpt.com "myshell-ai/OpenVoice · voice style control quality issues"
[8]: https://rhasspy.github.io/piper-samples/?utm_source=chatgpt.com "Piper Voice Samples"
[9]: https://pypi.org/project/piper-tts/?utm_source=chatgpt.com "piper-tts"
[10]: https://unix.stackexchange.com/questions/798238/resolving-unknown-option-model-when-using-piper-text-to-voice?utm_source=chatgpt.com "model' when using piper text to voice"
